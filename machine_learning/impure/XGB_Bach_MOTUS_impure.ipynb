{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building XGBoost Model for the CoEDC dataset using Bach's features, MOTUS categories, impure windows, optimise hyperparameters with 5-fold CV, and LOSO validation on best model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting parameters for notebook so can easily change as right at top of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of CPUs to use\n",
    "N_CPUS = 16\n",
    "\n",
    "# size of windows in seconds\n",
    "window_size = 5\n",
    "\n",
    "# pure windows or majority?\n",
    "PURE_WINDOWS = False\n",
    "\n",
    "# accelerometer to be analysed; valid values are 'acg', 'axivity' and 'sens'\n",
    "accelerometer = 'sens'\n",
    "\n",
    "# any participants to exclude. Note that if processed_data_dir exists, then this will be ignored\n",
    "PARTICIPANTS_TO_EXCLUDE = []\n",
    "\n",
    "values_to_drop_before = ['Unknown']\n",
    "values_to_drop_after = ['Other']\n",
    "\n",
    "DATA_DIR = \"src/dc_data/\"\n",
    "\n",
    "# name of directory to store processed data\n",
    "file_prefix = 'XGB_Bach_MOTUS'\n",
    "\n",
    "processed_data_dir = f'processed_MOTUS_{\"pure\" if PURE_WINDOWS else \"impure\"}_{accelerometer}_all_{window_size}s'\n",
    "\n",
    "results_dir = f'results/{file_prefix}_{\"pure\" if PURE_WINDOWS else \"impure\"}_{accelerometer}_all_{window_size}s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "home_directory = os.path.expanduser(\"~\")\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "data_dir = os.path.join(home_directory, DATA_DIR)\n",
    "map_dir = parent_dir\n",
    "\n",
    "import utils\n",
    "import plot\n",
    "import cf_matrix\n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create output directory if doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, map to motus, make windows and output to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_and_make_windows(datafiles):\n",
    "    # Function which given a list of datafiles, loads the data and makes windows for each and concatenates and returns\n",
    "\n",
    "    def worker(datafile):\n",
    "        print(\"\\nProcessing\", datafile)\n",
    "        data = utils.load_data(datafile, acc_prefix = accelerometer)\n",
    "        data = utils.map_to_new_classes(data, 'annotation', os.path.join(map_dir, 'motus_class_map.json'), verbose=True)\n",
    "        data = data[~data['annotation'].isin(values_to_drop_before)]\n",
    "        X, Y, T = utils.make_windows(data, winsec=window_size, sample_rate=30, dropna=False, verbose=True, drop_impure=PURE_WINDOWS)\n",
    "        mask = ~np.isin(Y, values_to_drop_after)\n",
    "        X, Y, T = X[mask], Y[mask], T[mask]\n",
    "        print(f'After dropping {values_to_drop_after}, there are {len(X)} windows left')\n",
    "        pid = os.path.basename(datafile).split(\".\")[0]  # participant ID\n",
    "        pid = np.asarray([pid] * len(X))\n",
    "        return X, Y, T, pid\n",
    "\n",
    "    results = []\n",
    "    for datafile in tqdm(datafiles):\n",
    "        if os.path.basename(datafile) in acc_missing:\n",
    "            print(\"\\nSkipping\", datafile)\n",
    "            continue\n",
    "        result = worker(datafile)\n",
    "        results.append(result)\n",
    "\n",
    "    X = np.concatenate([result[0] for result in results])\n",
    "    Y = np.concatenate([result[1] for result in results])\n",
    "    T = np.concatenate([result[2] for result in results])\n",
    "    pid = np.concatenate([result[3] for result in results])\n",
    "\n",
    "    return X, Y, T, pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (accelerometer == 'acg'):\n",
    "    ACC_MISSING = [23, 24, 25, 34, 36]\n",
    "elif (accelerometer == 'axivity'):\n",
    "    raise Exception(\"Axivity data not tested yet\")\n",
    "elif (accelerometer == 'sens'):\n",
    "    ACC_MISSING = []\n",
    "else:\n",
    "    raise Exception(\"Invalid accelerometer type\")\n",
    "\n",
    "acc_missing = [f'P{i:02d}.csv.gz' for i in ACC_MISSING]\n",
    "acc_missing.extend([f'P{i:02d}.csv.gz' for i in PARTICIPANTS_TO_EXCLUDE])\n",
    "\n",
    "# check if data directory exists\n",
    "if not os.path.exists(data_dir):\n",
    "    # raise an error\n",
    "    raise Exception(\"Data directory does not exist. Please create it and download the data.\")\n",
    "\n",
    "if os.path.exists(processed_data_dir):\n",
    "    print(\"Data already processed\")\n",
    "else:\n",
    "    # ------------------------------------------\n",
    "    # Process all files\n",
    "    # ------------------------------------------\n",
    "    datafiles =  os.path.join(data_dir, 'P[0-9][0-9].csv.gz')\n",
    "    X, Y, T, pid = load_all_and_make_windows(sorted(glob(datafiles)))\n",
    "    # Save arrays for future use\n",
    "    os.makedirs(processed_data_dir)\n",
    "    np.save(os.path.join(processed_data_dir, 'X.npy'), X)\n",
    "    np.save(os.path.join(processed_data_dir, 'Y.npy'), Y)\n",
    "    np.save(os.path.join(processed_data_dir, 'T.npy'), T)\n",
    "    np.save(os.path.join(processed_data_dir, 'pid.npy'), pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data we have just written out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed files\n",
    "X = np.load(os.path.join(processed_data_dir, 'X.npy'), mmap_mode='r')\n",
    "Y = np.load(os.path.join(processed_data_dir, 'Y.npy'))\n",
    "T = np.load(os.path.join(processed_data_dir, 'T.npy'))\n",
    "# pid is the participant id for each window\n",
    "pid = np.load(os.path.join(processed_data_dir, 'pid.npy'))\n",
    "\n",
    "# check if any NaNs in X\n",
    "if np.isnan(X).any():\n",
    "    print(\"NaNs found\")\n",
    "else:\n",
    "    print(\"No NaNs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of window_size windows for each activity class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLabel distribution (# windows)')\n",
    "print(pd.Series(Y).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save raw accelerometer data for each participant for some of the plotting analyses of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_accel_data = {}\n",
    "for participant in np.unique(pid):\n",
    "    participant_accel_data[participant] = X[pid == participant]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "Using features from Bach paper - https://journals.humankinetics.com/view/journals/jmpb/5/1/article-p24.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bach_features\n",
    "\n",
    "X_feats = pd.DataFrame([bach_features.bach_features(x, sample_rate=30) for x in tqdm(X)])\n",
    "print(f\"X_feats shape: {X_feats.shape}\")\n",
    "\n",
    "# store feature names as needed to label confusion matrix\n",
    "feats_names = X_feats.columns\n",
    "# convert X_feats to numpy array in preparation for training\n",
    "X_feats = np.asarray(X_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search to find optimised hyperparameters using 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_groups_in_cv_folds(X, y, groups, cv):\n",
    "    for fold, (_, test_index) in enumerate(cv.split(X, y, groups)):\n",
    "        print(f\"Fold_{fold} Test Participants: {np.unique(groups[test_index])}\")\n",
    "\n",
    "def check_unique_labels_in_cv_folds(X, y, groups, cv):\n",
    "    unique_labels = []\n",
    "\n",
    "    for fold, (_, test_index) in enumerate(cv.split(X, y, groups)):\n",
    "        fold_unique_labels = np.unique(y[test_index])\n",
    "        print(f\"Fold_{fold} Unique Labels: {fold_unique_labels}\")\n",
    "        unique_labels.append(set(fold_unique_labels))\n",
    "\n",
    "    # Check if all splits have the same set of unique labels\n",
    "    for i in range(1, len(unique_labels)):\n",
    "        if unique_labels[i] != unique_labels[0]:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "scoring = ['recall_macro', 'precision_macro', 'f1_macro', 'accuracy']\n",
    "\n",
    "# Define the parameter values that we want to search over\n",
    "\n",
    "param_grid = {\n",
    "              'eta': [0.1],\n",
    "              'subsample': [0.6],\n",
    "              'max_depth': [4,5, 6, 7, 8, 9, 10, 11, 12],\n",
    "              'n_estimators': [50, 60, 70, 80, 90, 100, 110],\n",
    "              'objective': ['multi:softprob'],\n",
    "              'seed': [42]}\n",
    "\n",
    "# Set to XGBoost model\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# XGBoost needs the labels to be integers starting from 0\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "print(le.classes_)\n",
    "\n",
    "\n",
    "# Do I need normalised features? The XGB author says no - see https://github.com/dmlc/xgboost/issues/357\n",
    "\n",
    "# Define GridSearchCV with StratifiedGroupKFold cross-validator\n",
    "sgkf = StratifiedGroupKFold(n_splits=5)\n",
    "\n",
    "print_test_groups_in_cv_folds(X_feats, Y, pid, sgkf)\n",
    "\n",
    "if not check_unique_labels_in_cv_folds(X_feats, Y, pid, sgkf):\n",
    "    raise Exception(\"Not all splits have the same set of unique labels\")\n",
    "\n",
    "# after discussions with Shing have decided to use recall_macro (is same as balanced accuracy) as discriminator\n",
    "grid = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=sgkf, scoring=scoring, refit='recall_macro', n_jobs=N_CPUS, verbose=3, return_train_score=True)\n",
    "\n",
    "# Fit model with data\n",
    "grid.fit(X_feats, Y, groups=pid)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put results in dataframe\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "# None get converted to NaN - turn them to string 'None' so easy to filter if needed\n",
    "results = results.where(pd.notnull(results), 'None')\n",
    "\n",
    "# and print out best parameters for each metric calculated\n",
    "for scorer in scoring:\n",
    "    # Find index of best score\n",
    "    index_of_best_score = np.argmax(results['mean_test_{}'.format(scorer)])\n",
    "    \n",
    "    # Find best params from best score index\n",
    "    best_params_for_scorer = results['params'][index_of_best_score]\n",
    "    # Get corresponding fit and score times\n",
    "    fit_time = results['mean_fit_time'][index_of_best_score]\n",
    "    score_time = results['mean_score_time'][index_of_best_score]\n",
    "    \n",
    "    # Print best params for each scoring\n",
    "    print(f'Score time: {score_time:.4f} - Best params for {scorer}:{best_params_for_scorer}')\n",
    "\n",
    "print('\\n{:<20} {:<20} {:<20} {:<20} {:<20}'.format('Scorer', *scoring))\n",
    "for scorer in scoring:\n",
    "    # Find index of best score\n",
    "    index_of_best_score = np.argmax(results['mean_test_{}'.format(scorer)])\n",
    "\n",
    "    # Calculate corresponding scores for best parameters\n",
    "    best_scores_for_scorer = [results['mean_test_{}'.format(scorer)][index_of_best_score] for scorer in scoring]\n",
    "\n",
    "    # Print best scores for each scorer\n",
    "    print('{:<20} {:<20.4f} {:<20.4f} {:<20.4f} {:<20.4f}'.format(scorer, *best_scores_for_scorer))\n",
    "\n",
    "print('\\nTraining results for comparison (check for overfitting)')\n",
    "for scorer in scoring:\n",
    "    # Find index of best score\n",
    "    index_of_best_score = np.argmax(results['mean_test_{}'.format(scorer)])\n",
    "\n",
    "    # Calculate corresponding scores for best parameters\n",
    "    best_scores_for_scorer = [results['mean_train_{}'.format(scorer)][index_of_best_score] for scorer in scoring]\n",
    "\n",
    "    # Print best scores for each scorer\n",
    "    print('{:<20} {:<20.4f} {:<20.4f} {:<20.4f} {:<20.4f}'.format(scorer, *best_scores_for_scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "cols_to_keep = ['params'] + \\\n",
    "                [col for col in results.columns if 'mean_test' in col or 'mean_train' in col]\n",
    "\n",
    "df_results = results[cols_to_keep]\n",
    "# Split the 'params' column into separate columns\n",
    "expanded_params = df_results['params'].apply(pd.Series)\n",
    "\n",
    "# Concatenate the expanded 'params' DataFrame with the original minus 'params'\n",
    "df_results_expanded = pd.concat([expanded_params, df_results.drop('params', axis=1)], axis=1)\n",
    "\n",
    "# Rename columns to remove 'mean_' prefix\n",
    "df_results_expanded.rename(columns=lambda x: x.replace('mean_', ''), inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_results_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present 3D plot of search. If more than 2 variables scanned then some will need to be held at fixed value or used for coloring of points\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# mask = results['param_class_weight'] == 'None'\n",
    "# filtered = results[mask]\n",
    "\n",
    "# create the plot with plotly\n",
    "fig = px.scatter_3d(results, \n",
    "                    x='param_max_depth',\n",
    "                    y='param_n_estimators',\n",
    "                    z='mean_test_recall_macro',\n",
    "                    color='mean_test_recall_macro',\n",
    "                    color_continuous_scale='viridis')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(data =\n",
    "    go.Contour(\n",
    "        x=results['param_max_depth'],\n",
    "        y=results['param_n_estimators'],\n",
    "        z=results['mean_test_recall_macro'],\n",
    "        colorscale='Viridis'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=500,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "# Add axis labels\n",
    "fig.update_xaxes(title_text='Max Depth')\n",
    "fig.update_yaxes(title_text='Number of Estimators')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a LOSO fit using optimised parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Get best params\n",
    "best_params = grid.best_params_\n",
    "\n",
    "# Initialize RFC with best params\n",
    "model = XGBClassifier(**best_params)\n",
    "\n",
    "# Initialize LeaveOneGroupOut\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Get labels and put in correct order\n",
    "# Note that labels in this case are numbers so need to convert back to strings, fix order and then convert back to numbers\n",
    "labels = np.unique(Y)\n",
    "n_labels = len(labels)\n",
    "labels_as_str = le.inverse_transform(labels)\n",
    "color_labels = list(plot.MOTUS_LABEL_COLORS.keys())\n",
    "ordered_labels = [label for label in color_labels if label in labels_as_str]\n",
    "labels_as_str = np.array(ordered_labels)\n",
    "labels = le.transform(np.array(ordered_labels))\n",
    "print(f'Labels as string: {labels_as_str}')\n",
    "print(f'Labels: {labels}')\n",
    "\n",
    "\n",
    "def train_fold(i, indices, X, X_feats, Y, pid, T):\n",
    "    train_index, test_index = indices\n",
    "    X_train, X_test = X_feats[train_index], X_feats[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    t_test = T[test_index]\n",
    "    X_raw_test = X[test_index]\n",
    "\n",
    "    # clone and fit the model\n",
    "    local_model = clone(model)\n",
    "    local_model.fit(X_train, y_train)\n",
    "    \n",
    "    # predict and calculate scores\n",
    "    y_pred = local_model.predict(X_test)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average='macro', zero_division=np.nan)\n",
    "    recall = recall_score(y_test, y_pred, average='macro', zero_division=np.nan)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=np.nan)\n",
    "    cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    conf_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    test_participant = np.unique(pid[test_index])[0]\n",
    "    print(f'Fold {i} Participant: {test_participant}, recall: {recall:.4f}, precision: {precision:.4f}, f1: {f1:.4f}, kappa: {cohen_kappa:.4f}, accuracy: {accuracy:.4f}')\n",
    "#    X_test_pd = pd.DataFrame(X_test, columns=feats_names)\n",
    "#    compare_fig = plot.plot_compare(t_test, y_test, y_pred, window_size, plot.MOTUS_LABEL_COLORS, participant=test_participant, trace=X_test_pd['xMean'])\n",
    "    \n",
    "    # save cm and figures to files\n",
    "    pd.DataFrame(conf_mat, index=labels_as_str, columns=labels_as_str).to_csv(os.path.join(results_dir, f'{test_participant}_cm.csv'))\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        cf_matrix.make_confusion_matrix(conf_mat, sum_stats=True, categories=labels_as_str, figsize=(n_labels+1,n_labels), filename=os.path.join(results_dir, f'{test_participant}_cm.png'))\n",
    "\n",
    "    compare_fig = plot.plot_compare(t_test, le.inverse_transform(y_test), le.inverse_transform(y_pred), window_size, plot.MOTUS_LABEL_COLORS, raw_traces=participant_accel_data[test_participant], y_extents=[-3, 3])\n",
    "    compare_fig.write_html(os.path.join(results_dir, f'{test_participant}_compare.html'), include_plotlyjs='cdn', full_html=False)\n",
    "    del compare_fig\n",
    "    \n",
    "    for label in labels_as_str:\n",
    "        failure_fig = plot.plot_failure_cases(X_raw_test, le.inverse_transform(y_test), le.inverse_transform(y_pred), t_test, label, sample_rate=30, n_samples=8)\n",
    "        failure_fig.write_html(os.path.join(results_dir, f'{test_participant}_ground_truth_{label}_failure.html'), include_plotlyjs='cdn', full_html=False)\n",
    "        del failure_fig\n",
    "    \n",
    "    for label in labels_as_str:\n",
    "        failure_fig = plot.plot_failure_cases(X_raw_test,le.inverse_transform(y_test), le.inverse_transform(y_pred), t_test, label, sample_rate=30, n_samples=8, swap_ground_and_predicted=True)\n",
    "        failure_fig.write_html(os.path.join(results_dir, f'{test_participant}_prediction_{label}_failure.html'), include_plotlyjs='cdn', full_html=False)\n",
    "        del failure_fig\n",
    "\n",
    "    return precision, recall, f1, cohen_kappa, accuracy, conf_mat, local_model\n",
    "\n",
    "def train_model(X, X_feats, Y, pid, T):\n",
    "    indices = list(logo.split(X_feats, Y, pid))\n",
    "    results = Parallel(n_jobs=N_CPUS)(delayed(train_fold)(i, index, X, X_feats, Y, pid, T) for i, index in enumerate(indices))\n",
    "\n",
    "    # Sort the results by recall_scores (which is the third item in each tuple)\n",
    "    results = sorted(results, key=lambda x: x[2])\n",
    "    return results\n",
    "\n",
    "results = train_model(X, X_feats, Y, pid, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "# unpack results\n",
    "precision_scores, recall_scores, f1_scores, cohen_kappa_scores, accuracy_scores, confusion_matrices, models = zip(*results)\n",
    "\n",
    "# calculate mean scores\n",
    "mean_precision = np.mean(precision_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_cohen_kappa = np.mean(cohen_kappa_scores)\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "# calculate standard deviations\n",
    "std_precision = np.std(precision_scores)\n",
    "std_recall = np.std(recall_scores)\n",
    "std_f1 = np.std(f1_scores)\n",
    "std_cohen_kappa = np.std(cohen_kappa_scores)\n",
    "std_accuracy = np.std(accuracy_scores)\n",
    "\n",
    "print(\"\\nAverages across all participants\")\n",
    "print(f\"Recall:{mean_recall:.3f}({std_recall:.3f}) Precision:{mean_precision:.3f}({std_precision:.3f}) F1:{mean_f1:.3f}({std_f1:.3f}) Kappa:{mean_cohen_kappa:.3f}({std_cohen_kappa:.3f}) Accuracy:{mean_accuracy:.3f}({std_accuracy:.3f})\")\n",
    "\n",
    "# calculate combined confusion matrix\n",
    "combined_conf_mat = np.sum(np.array(confusion_matrices), axis=0)\n",
    "cf_matrix.make_confusion_matrix(combined_conf_mat, sum_stats=True, categories=labels_as_str, figsize=(n_labels+1,n_labels))\n",
    "\n",
    "# write to files\n",
    "pd.DataFrame(combined_conf_mat, index=labels_as_str, columns=labels_as_str).to_csv(os.path.join(results_dir, 'combined_cm.csv'))\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    cf_matrix.make_confusion_matrix(combined_conf_mat, sum_stats=True, categories=labels_as_str, figsize=(n_labels+1,n_labels), filename=os.path.join(results_dir, 'combined_cm.png'))\n",
    "\n",
    "precision_macro, recall_macro, f1_macro, accuracy, precision, recall, f1 = cf_matrix.calculate_metrics(combined_conf_mat)\n",
    "\n",
    "# Create a PrettyTable object\n",
    "table = PrettyTable()\n",
    "\n",
    "# Set the column names\n",
    "table.field_names = [\"\", \"recall\", \"precision\", \"f1-score\"]\n",
    "\n",
    "# Add the metrics for each class\n",
    "for i, class_ in enumerate(labels_as_str):\n",
    "    table.add_row([class_, f\"{recall[i]:.3f}\", f\"{precision[i]:.3f}\", f\"{f1[i]:.3f}\"])\n",
    "\n",
    "# Add an empty row\n",
    "table.add_row([\"\", \"\", \"\", \"\"])\n",
    "\n",
    "# calculate standard deviations\n",
    "std_precision_macro = np.std(precision)\n",
    "std_recall_macro = np.std(recall)\n",
    "std_f1_macro = np.std(f1)\n",
    "\n",
    "# Add the averages\n",
    "table.add_row([\"Macro Average\", f\"{recall_macro:.3f}({std_recall_macro:.3f})\", f\"{precision_macro:.3f}({std_precision_macro:.3f})\", f\"{f1_macro:.3f}({std_f1_macro:.3f})\"])\n",
    "\n",
    "# Add the accuracy\n",
    "table.add_row([\"Accuracy\", f\"{accuracy:.3f}\", \"\", \"\"])\n",
    "\n",
    "# Print the table\n",
    "print(table)\n",
    "\n",
    "# Get the HTML representation of the table\n",
    "html_table = table.get_html_string()\n",
    "\n",
    "# write stats to html file\n",
    "with open(os.path.join(results_dir, 'metrics.html'), 'w') as file:\n",
    "    file.write('<style>\\n')\n",
    "    file.write('table {border-collapse: collapse; font-family: sans-serif;}\\n')\n",
    "    file.write('th, td {border: 1px solid black; padding: 8px; text-align: left;}\\n')\n",
    "    file.write('th {background-color: #f2f2f2;}\\n')\n",
    "    file.write('</style>\\n')\n",
    "    file.write(html_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now fit full model and look at confusion matrix\n",
    "### if not overfitted should be similar to LOSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = clone(model)\n",
    "final_model.fit(X_feats, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model as is final model fitted on all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump((final_model, le, labels, labels_as_str), os.path.join(results_dir, 'final_model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(final_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nClassifier performance on all of our data')\n",
    "Y_pred = final_model.predict(X_feats)\n",
    "print(classification_report(Y, Y_pred, labels=labels, target_names=labels_as_str))\n",
    "\n",
    "cm_train = confusion_matrix(Y, Y_pred, labels=labels)\n",
    "pd.DataFrame(cm_train, index=labels_as_str, columns=labels_as_str).to_csv(os.path.join(results_dir, 'cm_train_all.csv'))\n",
    "cf_matrix.make_confusion_matrix(cm_train, sum_stats=False, categories=labels_as_str, figsize=(n_labels+1,n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress results directory using 7z format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from py7zr import FILTER_LZMA2, SevenZipFile\n",
    "\n",
    "def make_7z(output_filename, source_dir):\n",
    "    filters = [{\"id\": FILTER_LZMA2, \"preset\": 9}]  # Use LZMA2 with maximum compression level\n",
    "    with SevenZipFile(output_filename, mode='w', filters=filters) as z:\n",
    "        z.writeall(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "# Create the 7z file in the parent directory of results_dir\n",
    "sevenz_filename = os.path.join(os.path.dirname(results_dir), os.path.basename(results_dir) + '.7z')\n",
    "make_7z(sevenz_filename, results_dir)\n",
    "\n",
    "# Now it's safe to remove the original directory\n",
    "shutil.rmtree(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the correlation between features in the RF model\n",
    "Feature correlation gives an indication of how a pair of features are associated with one another. A pair of features with a high correlation coefficient (close to 1) tends to make an inefficient model, as we only need one of these features to extract the information required for classification. A visualisation of the correlation of all extracted features can be seen in a correlation matrix, and any pairs of features with high values can be removed from the feature extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feats_df = pd.DataFrame(X_feats, columns=feats_names)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(X_feats_df.corr().abs(), cmap='coolwarm').set_title('Correlation matrix of extracted features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the most important features\n",
    "Feature importance is an explainable AI technique to reveal the relative significance of individual features on model outputs. There are many different methods that can be used to determine feature importance, however in this notebook, we will use GINI importance. When looking to optimise feature extraction (less compute power and time, better model performance), features with lowest importance should be removed first.\n",
    "\n",
    "#### GINI Importance\n",
    "GINI importance is a feature importance method that can be extracted directly from the BalancedRandomForestClassifier class. More information on how exactly it works can be found [here](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-10-213).\n",
    "Note: GINI importance is known to perform poorly with highly correlated features. If relying only on GINI importance to determine which features to remove, highly correlated features should be removed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of top features\n",
    "n = 20\n",
    "\n",
    "# Get the indices of the top n features\n",
    "top_n_indices = np.argsort(final_model.feature_importances_)[-n:]\n",
    "\n",
    "# Select the top n feature importances and their names\n",
    "top_n_importances = final_model.feature_importances_[top_n_indices]\n",
    "top_n_names = np.array(feats_names)[top_n_indices]\n",
    "\n",
    "# Plot the top n features in descending order of importance\n",
    "plt.figure()\n",
    "sns.barplot(x=top_n_importances, y=top_n_names, order=top_n_names[::-1]).set(title=f\"Top {n} GINI Feature importance\");\n",
    "\n",
    "# Plot them all\n",
    "plt.figure(figsize=(6, 30))\n",
    "sns.barplot(x=final_model.feature_importances_, y=feats_names).set(title=\"GINI Feature importance\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
