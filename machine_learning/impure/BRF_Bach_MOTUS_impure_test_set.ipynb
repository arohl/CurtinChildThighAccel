{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing BRF Model trained on CoEDC dataset using Bach's features, MOTUS categories, impure windows, optimise hyperparameters with 5-fold CV, and LOSO validation on best model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting parameters for notebook so can easily change as right at top of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of windows in seconds\n",
    "window_size = 5\n",
    "\n",
    "# pure windows or majority?\n",
    "PURE_WINDOWS = False\n",
    "\n",
    "# accelerometer to be analysed; valid values are 'acg', 'axivity' and 'sens'\n",
    "accelerometer = 'sens'\n",
    "\n",
    "# any participants to exclude. Note that if processed_data_dir exists, then this will be ignored\n",
    "PARTICIPANTS_TO_EXCLUDE = []\n",
    "\n",
    "values_to_drop_before = ['Unknown']\n",
    "values_to_drop_after = ['Other']\n",
    "\n",
    "TEST_DATA_DIR = 'src/dc_data/test'\n",
    "\n",
    "file_prefix = 'BRF_Bach_MOTUS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import load\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "home_directory = os.path.expanduser(\"~\")\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "test_data_dir = os.path.join(home_directory, TEST_DATA_DIR)\n",
    "map_dir = parent_dir\n",
    "\n",
    "import utils\n",
    "import plot\n",
    "import cf_matrix\n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_and_make_windows(datafiles):\n",
    "    # Function which given a list of datafiles, loads the data and makes windows for each and concatenates and returns\n",
    "\n",
    "    def worker(datafile):\n",
    "        print(\"\\nProcessing\", datafile)\n",
    "        data = utils.load_data(datafile, acc_prefix = accelerometer)\n",
    "        data = utils.map_to_new_classes(data, 'annotation', os.path.join(map_dir, 'motus_class_map.json'), verbose=True)\n",
    "        data = data[~data['annotation'].isin(values_to_drop_before)]\n",
    "        X, Y, T = utils.make_windows(data, winsec=window_size, sample_rate=30, dropna=False, verbose=True, drop_impure=PURE_WINDOWS)\n",
    "        mask = ~np.isin(Y, values_to_drop_after)\n",
    "        X, Y, T = X[mask], Y[mask], T[mask]\n",
    "        print(f'After dropping {values_to_drop_after}, there are {len(X)} windows left')\n",
    "        pid = os.path.basename(datafile).split(\".\")[0]  # participant ID\n",
    "        pid = np.asarray([pid] * len(X))\n",
    "        return X, Y, T, pid\n",
    "\n",
    "    results = []\n",
    "    for datafile in tqdm(datafiles):\n",
    "        if os.path.basename(datafile) in test_acc_missing:\n",
    "            print(\"\\nSkipping\", datafile)\n",
    "            continue\n",
    "        result = worker(datafile)\n",
    "        results.append(result)\n",
    "\n",
    "    X = np.concatenate([result[0] for result in results])\n",
    "    Y = np.concatenate([result[1] for result in results])\n",
    "    T = np.concatenate([result[2] for result in results])\n",
    "    pid = np.concatenate([result[3] for result in results])\n",
    "\n",
    "    return X, Y, T, pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PARTICIPANTS_TO_EXCLUDE = []\n",
    "\n",
    "if (accelerometer == 'acg'):\n",
    "    TEST_ACC_MISSING = [26, 27, 37]\n",
    "elif (accelerometer == 'axivity'):\n",
    "    raise Exception(\"Axivity data not tested yet\")\n",
    "elif (accelerometer == 'sens'):\n",
    "    TEST_ACC_MISSING = []\n",
    "\n",
    "test_acc_missing = [f'P{i:02d}.csv.gz' for i in TEST_ACC_MISSING]\n",
    "test_acc_missing.extend([f'P{i:02d}.csv.gz' for i in TEST_PARTICIPANTS_TO_EXCLUDE])\n",
    "\n",
    "# check if test data directory exists\n",
    "if not os.path.exists(test_data_dir):\n",
    "    # raise an error\n",
    "    raise Exception(\"Test data directory does not exist. Please create it and download the test data.\")\n",
    "\n",
    "test_data_files =  os.path.join(test_data_dir, 'P[0-9][0-9].csv.gz')\n",
    "\n",
    "X_test, Y_test, T_test, pid_test = load_all_and_make_windows(sorted(glob(test_data_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print label distribution and calculate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLabel distribution (# windows)')\n",
    "print(pd.Series(Y_test).value_counts())\n",
    "\n",
    "import bach_features\n",
    "X_test_feats = pd.DataFrame([bach_features.bach_features(x, sample_rate=30) for x in tqdm(X_test)])\n",
    "print(f\"X_test_feats shape: {X_test_feats.shape}\")\n",
    "\n",
    "# convert X_test_feats to numpy array in preparation for classification\n",
    "X_test_feats = np.asarray(X_test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(final_model, labels) = load(os.path.join(current_dir, f'{file_prefix}_final_model.pkl'))\n",
    "n_labels = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_dir = 'results'\n",
    "\n",
    "if not os.path.exists(test_results_dir):\n",
    "    os.makedirs(test_results_dir)\n",
    "\n",
    "Y_test_pred = final_model.predict(X_test_feats)\n",
    "\n",
    "print('\\nPer participant classification report')\n",
    "print(utils.per_participant_metrics(Y_test, Y_test_pred, pid_test))\n",
    "\n",
    "print('\\nClassifier performance on our test data')\n",
    "print(classification_report(Y_test, Y_test_pred))\n",
    "\n",
    "cm_test = confusion_matrix(Y_test, Y_test_pred, labels=labels)\n",
    "cm_name = f'BRF_{\"pure\" if PURE_WINDOWS else \"impure\"}_{window_size}s_cm_test.csv'\n",
    "pd.DataFrame(cm_test, index=labels, columns=labels).to_csv(os.path.join(test_results_dir, cm_name))\n",
    "cf_matrix.make_confusion_matrix(cm_test, sum_stats=True, categories=labels, figsize=(n_labels+1,n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun model but on pure test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure windows or majority?\n",
    "PURE_WINDOWS = True\n",
    "X_test, Y_test, T_test, pid_test = load_all_and_make_windows(sorted(glob(test_data_files)))\n",
    "\n",
    "X_test_feats = pd.DataFrame([bach_features.bach_features(x, sample_rate=30) for x in tqdm(X_test)])\n",
    "print(f\"X_test_feats shape: {X_test_feats.shape}\")\n",
    "\n",
    "# convert X_test_feats to numpy array in preparation for classification\n",
    "X_test_feats = np.asarray(X_test_feats)\n",
    "\n",
    "Y_test_pred = final_model.predict(X_test_feats)\n",
    "\n",
    "print('\\nPer participant classification report')\n",
    "print(utils.per_participant_metrics(Y_test, Y_test_pred, pid_test))\n",
    "\n",
    "print('\\nClassifier performance on our test data')\n",
    "print(classification_report(Y_test, Y_test_pred))\n",
    "\n",
    "cm_test = confusion_matrix(Y_test, Y_test_pred, labels=labels)\n",
    "cm_name = f'BRF_{\"pure\" if PURE_WINDOWS else \"impure\"}_{window_size}s_cm_test.csv'\n",
    "pd.DataFrame(cm_test, index=labels, columns=labels).to_csv(os.path.join(test_results_dir, cm_name))\n",
    "cf_matrix.make_confusion_matrix(cm_test, sum_stats=True, categories=labels, figsize=(n_labels+1,n_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
